---
title: "Accelerating Part-Scale Simulation in Liquid Metal Jet Additive Manufacturing via Operator Learning"
collection: publications
permalink: /publication/2022-01-01-paper-1
excerpt: 
date: 2022-01-01
venue: 'AI-Based Design and Manufacturing (ADAM) workshop at the 36th AAAI Conference on Artificial Intelligence (2022)'
paperurl: 'https://arxiv.org/abs/2202.03665'
citation: '<b>S. Taverniers</b>, S. Korneev, K.M. Pietrzyk, and M. Behandish. Accelerating Part-Scale Simulation in Liquid Metal Jet Additive Manufacturing via Operator Learning. <i>36th AAAI Conference on Artificial Intelligence.</i>, (2022).'
---

## Abstract

Predicting part quality for additive manufacturing (AM) processes requires high-fidelity numerical simulation of partial differential equations (PDEs) governing process multiphysics on a scale of minimum manufacturable features. This makes part-scale predictions computationally demanding, especially when they require many small-scale simulations. We consider drop-on-demand liquid metal jetting (LMJ) as an illustrative example of such computational complexity. A model describing droplet coalescence for LMJ may include coupled incompressible fluid flow, heat transfer, and phase change equations. Numerically solving these equations becomes prohibitively expensive when simulating the build process for a full part consisting of thousands to millions of droplets. Reduced-order models (ROMs) based on neural networks (NN) or k-nearest neighbor (kNN) algorithms have been built to replace the original physics-based solver and are computationally tractable for part-level simulations. However, their quick inference capabilities often come at the expense of accuracy, robustness, and generalizability. We apply an operator learning (OL) approach to learn a mapping between initial and final states of the droplet coalescence process for enabling rapid and accurate part-scale build simulation. Preliminary results suggest that OL requires order-of-magnitude fewer data points than a kNN approach and is generalizable beyond the training set while achieving similar prediction error.